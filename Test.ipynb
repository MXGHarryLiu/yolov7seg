{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.torch_utils import select_device\n",
    "from utils.general import (check_img_size, scale_coords, non_max_suppression, cv2)\n",
    "from utils.segment.general import process_mask\n",
    "\n",
    "from utils.augmentations import letterbox\n",
    "import numpy as np\n",
    "\n",
    "weights=Path().resolve() / 'best.pt'\n",
    "data=Path(r\"D:\\QuickSegData\\Nuclei2\") / 'coco128.yaml'  # dataset.yaml path\n",
    "source='D:/QuickSegData/Nuclei2/test/images/0a849e0eb15faa8a6d7329c3dd66aabe9a294cccb52ed30a90c8ca99092ae732.png'  # file/dir/URL/glob, 0 for webcam\n",
    "device='cpu'  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "dnn=False  # use OpenCV DNN for ONNX inference\n",
    "half=False  # use FP16 half-precision inference\n",
    "imgsz=(640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-5-2 Python-3.11.3 torch-2.0.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 224 layers, 7398422 parameters, 0 gradients, 25.7 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = select_device(device)\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thres=0.25  # confidence threshold\n",
    "iou_thres=0.45  # NMS IOU threshold\n",
    "max_det=1000  # maximum detections per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "bs = 1  # batch_size\n",
    "model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path, im, im0s, vid_cap, s = next(iter(dataset)) # assume only one item\n",
    "im0 = cv2.imread(source)\n",
    "im = letterbox(im0, imgsz, stride=stride, auto=True)[0]  # padded resize\n",
    "im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "im = np.ascontiguousarray(im)  # contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = torch.from_numpy(im).to(device)\n",
    "im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "if len(im.shape) == 3:\n",
    "    im = im[None]  # expand for batch dim\n",
    "\n",
    "pred, out = model(im, augment=False, visualize=False)\n",
    "proto = out[1]\n",
    "\n",
    "classes = None # filter by class: --class 0, or --class 0 2 3 \n",
    "agnostic_nms=False,  # class-agnostic NMS\n",
    "pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det, nm=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = pred[0]\n",
    "i = 0\n",
    "# p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "# if len(det):\n",
    "masks = process_mask(proto[i], det[:, 6:], det[:, :4], im.shape[2:], upsample=True)  # HWC\n",
    "\n",
    "# # Rescale boxes from img_size to im0 size\n",
    "# det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "# # Print results\n",
    "# for c in det[:, 5].unique():\n",
    "#     n = (det[:, 5] == c).sum()  # detections per class\n",
    "#     s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
